{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "from louvain_communities.louvain import detect_communities, modularity\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "seq_path = \"mRNA_sequences.csv\"\n",
    "labels_path_minus = \"A_minus_normalized_levels.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(print_model_summary=False):\n",
    "    \"\"\"\n",
    "    creates the tensorflow model\n",
    "    \"\"\"\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv1D(64, 10, activation='relu', input_shape=(110, 4)))\n",
    "    model.add(layers.MaxPooling1D(2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.mean_squared_error,\n",
    "                  metrics=[tf.keras.losses.mean_squared_error])\n",
    "    if print_model_summary:\n",
    "        print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "def one_hot_encoding(seq):\n",
    "    \"\"\"\n",
    "    one hot encoding function - convet sequence of lentgh L into (L, 4) one-hot matrix\n",
    "    \"\"\"\n",
    "    mapping = dict(zip(\"ACGT\", range(4)))    \n",
    "    seq2 = [mapping[i] for i in seq]\n",
    "\n",
    "    return np.eye(4)[seq2].astype('uint8')\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    # load the dataset and create the one-hot matrices\n",
    "    seq = pd.read_csv(seq_path)[\"seq\"].values.tolist()\n",
    "    features_list = [one_hot_encoding(x) for x in seq]\n",
    "    features = np.asarray(features_list, dtype=\"uint8\")\n",
    "\n",
    "    labels = pd.read_csv(labels_path_minus)\n",
    "    labels = labels[[\"{}h\".format(i) for i in [1,2,3,4,5,6,7,8,10]]].values\n",
    "\n",
    "    # convert the mRNA levels into degradation rate\n",
    "    deg_rates = []\n",
    "    for i in range(len(labels)):\n",
    "        reg = LinearRegression()\n",
    "        reg.fit(np.array([1, 2, 3, 4, 5, 6, 7, 8, 10])[:, np.newaxis], labels[i, :][:, np.newaxis])\n",
    "        deg_rates.append(reg.coef_)\n",
    "        \n",
    "    deg_rates = np.squeeze(np.array(deg_rates))\n",
    "\n",
    "    return features, deg_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmer_link_graph(k=20, l=110, seq_file_path=\"mRNA_sequences.csv\"):\n",
    "    \"\"\"\n",
    "    creates a graph where seqences are nodes and link is formes if there is k-mer share\n",
    "\n",
    "    k : k-mer length. Default: 20\n",
    "    l : sequence fixed length. Default: 110\n",
    "    seq_file_path: str\n",
    "    \"\"\"\n",
    "    seq_list = pd.read_csv(seq_file_path)[\"seq\"].values.tolist()\n",
    "    hash_map = {}\n",
    "    l = 110 # sequence fixed length\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(len(seq_list)))\n",
    "\n",
    "    #passing all k-mer in the seqences and create the edges accordingly \n",
    "    for i,seq in enumerate(seq_list):\n",
    "        for j in range(l-k+1):\n",
    "            kmer=seq[j:j+k]\n",
    "            if kmer in hash_map:\n",
    "                hash_map[kmer].add(i)\n",
    "                edges_list = [(i,m) for m in hash_map[kmer]]\n",
    "                G.add_edges_from(edges_list)\n",
    "            else:\n",
    "                hash_map[kmer] = {i}\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "def kmer_link_graph_different_weights(ks=(15, 16, 17, 18, 19, 20),\n",
    "                                      l=110,\n",
    "                                      seq_file_path=\"mRNA_sequences.csv\"):\n",
    "    \"\"\"\n",
    "    creates a graph where seqences are nodes and link is formes if\n",
    "    there is k-mer share of minium length ks[0].\n",
    "    Form different weight for different length: ks[0]/ks[-1], ks[1]/ks[-1], ..., ks[-1]/ks[-1]=1\n",
    "    ks : k-mer lengths. Default: (15, 16, 17, 18, 19, 20)\n",
    "    l : sequence fixed length. Default: 110\n",
    "    seq_file_path: str\n",
    "    \"\"\"\n",
    "    k_max = max(ks)\n",
    "    seq_list = pd.read_csv(seq_file_path)[\"seq\"].values.tolist()\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(len(seq_list)))\n",
    "    for k in ks:\n",
    "        hash_map = {}\n",
    "        #passing all k-mer in the seqences and create the edges accordingly \n",
    "        for i,seq in enumerate(seq_list):\n",
    "            for j in range(l-k+1):\n",
    "                kmer=seq[j:j+k]\n",
    "                if kmer in hash_map:\n",
    "                    hash_map[kmer].add(i)\n",
    "                    edges_list = [(i,m) for m in hash_map[kmer]]\n",
    "                    G.add_edges_from(edges_list, weight=k/k_max)\n",
    "                else:\n",
    "                    hash_map[kmer] = {i}\n",
    "\n",
    "        return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_parition_to_train_and_test(partition, train_size=0.8, error_in_size=0.001):\n",
    "    samples_size = sum(len(cluster) for cluster in partition)\n",
    "    train_size = 0.8\n",
    "    partition_temp = copy.deepcopy(partition)\n",
    "    train_indices = []\n",
    "    while len(train_indices) < (train_size - error_in_size) * samples_size:\n",
    "        i = random.randint(0, len(partition_temp)-1)\n",
    "        if len(train_indices) + len(partition_temp[i]) < (train_size + error_in_size) * samples_size:\n",
    "            cluster_indices = partition_temp.pop(i)\n",
    "            train_indices.extend(cluster_indices)\n",
    "    test_indices = [index for cluster in partition_temp for index in cluster]\n",
    "    print(\"train size:\", len(train_indices))\n",
    "    print(\"test size:\", len(test_indices))\n",
    "\n",
    "    return train_indices, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_train_test_data_from_partition(features, deg_rates, train_indices, test_indices):\n",
    "    train_x = features[train_indices]\n",
    "    train_y = deg_rates[train_indices]\n",
    "    test_x = features[test_indices]\n",
    "    test_y = deg_rates[test_indices]\n",
    "\n",
    "    return train_x, test_x, train_y, test_y\n",
    "\n",
    "\n",
    "def create_train_and_evaluate_model(train_x, test_x, train_y, test_y, verbose=0):\n",
    "    \"\"\"\n",
    "    returns the train model and the Pearson score on the test set\n",
    "    \"\"\"\n",
    "    model = cnn_model()\n",
    "    model.fit(train_x,\n",
    "              train_y,\n",
    "              #validation_data=(validation_x, validation_y),\n",
    "              shuffle=True,\n",
    "              epochs=10,                             \n",
    "              batch_size= 64,\n",
    "              verbose=verbose\n",
    "              )\n",
    "    test_y_predict = np.squeeze(model.predict(test_x))\n",
    "\n",
    "    score = stats.pearsonr(test_y, test_y_predict)\n",
    "\n",
    "    return model, score[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expriment_train_test(partition, random_train=False):\n",
    "    features, deg_rates = load_data()\n",
    "    louvain_partition_scores, random_partition_scores = [], []\n",
    "    num_of_expriments, num_of_trains = 10, 10\n",
    "    for i in range(num_of_expriments):\n",
    "        print(\"louvain partition training:\")\n",
    "        train_indices, test_indices = divide_parition_to_train_and_test(partition, train_size=0.8)\n",
    "        for j in range(num_of_trains):\n",
    "            train_x, test_x, train_y, test_y = extract_train_test_data_from_partition(features, deg_rates, train_indices, test_indices)\n",
    "            model, score = create_train_and_evaluate_model(train_x, test_x, train_y, test_y)\n",
    "            louvain_partition_scores.append(score)\n",
    "        if random_train:\n",
    "            print(\"random partition training:\")\n",
    "            train_x, test_x, train_y, test_y = train_test_split(features, deg_rates, test_size=0.2, random_state=i)\n",
    "            for j in range(num_of_trains):\n",
    "                model, score = create_train_and_evaluate_model(train_x, test_x, train_y, test_y)\n",
    "                random_partition_scores.append(score)\n",
    "        print(\"finished expriment\", i)\n",
    "\n",
    "        louvain_partition_scores_arr = np.array(louvain_partition_scores)\n",
    "        louvain_partition_scores_arr = louvain_partition_scores_arr[~np.isnan(louvain_partition_scores_arr)]\n",
    "        print(\"Louvain partition training:\")\n",
    "        print(\"average Pearson:\", np.mean(louvain_partition_scores_arr))\n",
    "        if random_train:\n",
    "            random_partition_scores_arr = np.array(random_partition_scores)\n",
    "            random_partition_scores_arr = random_partition_scores_arr[~np.isnan(random_partition_scores_arr)]\n",
    "            print(\"Random partition training:\")\n",
    "            print(\"average Pearson:\", np.mean(random_partition_scores_arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single k-mer length link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "524752 edges\n"
     ]
    }
   ],
   "source": [
    "G = kmer_link_graph()\n",
    "print(G.size(), \"edges\")\n",
    "partition = detect_communities(G, randomized=True)\n",
    "print(\"Modularity for best partition:\", modularity(G, partition))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expriment_train_test(partition, random_train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different_weights for different k-mer length links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = kmer_link_graph_different_weights()\n",
    "print(G.size(), \"edges\")\n",
    "partition = detect_communities(G, randomized=True)\n",
    "print(\"Modularity for best partition:\", modularity(G, partition))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expriment_train_test(partition, random_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph based kmer counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "###############kmer count######################\n",
    "#kmer utilities functions \n",
    "def CountKmer(seq, k):\n",
    "    kFreq = {}\n",
    "    for i in range (0, len(seq)-k+1):\n",
    "        kmer = seq [i:i+k]\n",
    "        if kmer in kFreq:\n",
    "            kFreq [kmer] += 1\n",
    "        else:\n",
    "            kFreq [kmer] = 1\n",
    "    return kFreq\n",
    "\n",
    "def retutnAllKmers(min_kmer_length, max_kmer_length):\n",
    "    kmer_list = []\n",
    "    for i in range (min_kmer_length, max_kmer_length+1):\n",
    "        kmer_list = kmer_list + [''.join(c) for c in product('ACGT', repeat=i)]\n",
    "    return kmer_list\n",
    "\n",
    "def createFeturesVector(allKmers, seqkMerCounter):\n",
    "    AllKmersSize = len(allKmers)\n",
    "    KmerCounterArray = np.zeros((AllKmersSize,1))\n",
    "    for i in range (0, AllKmersSize):\n",
    "        if allKmers[i] in seqkMerCounter:\n",
    "              KmerCounterArray[i] = seqkMerCounter[allKmers[i]]\n",
    "    return KmerCounterArray\n",
    "\n",
    "def createFeturesVectorsForAllSeq(allKmers, min_kmer_length, max_kmer_length, sequences):\n",
    "    num_of_kmers = len(allKmers)\n",
    "    num_of_sequences = len(sequences)\n",
    "    FeturesVectorsOfAllSeq = np.zeros((num_of_sequences, num_of_kmers, 1), dtype='int8')\n",
    "    for i in range(num_of_sequences): \n",
    "        seq = sequences [i]\n",
    "        seqkMerCounter = {}\n",
    "        for j in range (min_kmer_length, max_kmer_length+1):\n",
    "            seqkMerCounter = {**seqkMerCounter, **CountKmer(seq, j)}\n",
    "        FeturesVectorsOfAllSeq[i] = createFeturesVector(allKmers, seqkMerCounter)\n",
    "    return FeturesVectorsOfAllSeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset and create the k-mer counting matrix\n",
    "seq = pd.read_csv(seq_path)[\"seq\"].values.tolist()\n",
    "allKmers = retutnAllKmers(4, 7)\n",
    "FeturesVectorsOfAllSeq = createFeturesVectorsForAllSeq(allKmers, 4, 7, seq)\n",
    "FeturesVectorsOfAllSeq = np.squeeze(FeturesVectorsOfAllSeq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(n_components=100)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce matrix dimension\n",
    "pca = PCA(n_components=100)\n",
    "pca.fit(FeturesVectorsOfAllSeq)\n",
    "FeturesVectorsOfAllSeq_pca = pca.transform(FeturesVectorsOfAllSeq)\n",
    "\n",
    "# bulid the graph matrix using kneighbors_graph based on Euclidean distace\n",
    "A = kneighbors_graph(FeturesVectorsOfAllSeq_pca, 10, mode=\"distance\")\n",
    "A[A.nonzero()] = 1 / (A[A.nonzero()] + 1) # convert to similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph(A)\n",
    "print(G.size(), \"edges\")\n",
    "partition = detect_communities(G, randomized=True)\n",
    "print(\"Modularity for best partition:\", modularity(G, partition))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expriment_train_test(partition, random_train=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ofir_env",
   "language": "python",
   "name": "ofir_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
